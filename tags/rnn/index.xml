<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>RNN on Gordon&#39;s Blog</title>
    <link>https://gordongwb.github.io/tags/rnn/</link>
    <description>Recent content in RNN on Gordon&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Dec 2019 21:28:47 +0800</lastBuildDate>
    
	<atom:link href="https://gordongwb.github.io/tags/rnn/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>LSTM Basic_Introduction</title>
      <link>https://gordongwb.github.io/tech/lstm-basic-introduction/</link>
      <pubDate>Wed, 25 Dec 2019 21:28:47 +0800</pubDate>
      
      <guid>https://gordongwb.github.io/tech/lstm-basic-introduction/</guid>
      <description>LSTM 由于梯度消失/梯度爆炸的问题传统RNN在实际中很难处理长期依赖，而LSTM（Long Short Term Memory）则绕开了这些问题依然可以从语料中学习到长期依赖关系。
传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作,如图：
LSTM每个循环的模块内又有4层结构:3个sigmoid层，1个tanh层，如图：
LSTM内部结构详解 LSTM的关键是细胞状态C，一条水平线贯穿于图形的上方，这条线上只有些少量的线性操作，信息在上面流传很容易保持。
第一层是个忘记层，决定细胞状态中丢弃什么信息。把ht−1和xt拼接起来，传给一个sigmoid函数，该函数输出0到1之间的值，这个值乘到细胞状态Ct−1上去。sigmoid函数的输出值直接决定了状态信息保留多少。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。
上一步的细胞状态Ct−1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项C~t，tanh的输出在[-1,1]上，说明细胞状态在某些维度上需要加强，在某些维度上需要减弱；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。
现在可以让旧的细胞状态Ct−1与ft（f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分it∗C~t（i是input输入门的意思），这就生成了新的细胞状态CtCt。
最后该决定输出什么了。输出值跟细胞状态有关，把Ct输给一个tanh函数得到输出值的候选项。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。
LSTM 详解 单层ＬＳＴＭ结构如下
cell = tf.contrib.rnn.LSTMBlockCell(hidden_size, forget_bias=0.0)	outputs = [] state = self._initial_state # state with tf.variable_scope(&amp;#34;RNN&amp;#34;): for time_step in range(num_steps): if time_step &amp;gt; 0: tf.get_variable_scope().reuse_variables() # cell_output: [batch_size,hidden_size] (cell_output, state) = cell(inputs[:,time_step,:], state) # outputs: a list: num_steps elements of shape [batch_size,hidden_size] outputs.append(cell_output) # output: first to shape:[batch_size,num_steps*hidden_size] and the first row is the data of the first sentense # and then reshpae to shape: [batch_size*num_steps,hidden_size], first num_steps rows is a sentense output = tf.</description>
    </item>
    
  </channel>
</rss>