<!DOCTYPE html>
<html lang="en-us">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="Gordon.Kwok">
  
  
  
  <link rel="prev" href="https://gordongwb.github.io/2019/cnn-basic-introduction/" />
  <link rel="next" href="https://gordongwb.github.io/2019/ctc-loss-basic-introduction/" />
  <link rel="canonical" href="https://gordongwb.github.io/2019/lstm-basic-introduction/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           LSTM Basic_Introduction | Gordon&#39;s Blog
       
  </title>
  <meta name="title" content="LSTM Basic_Introduction | Gordon&#39;s Blog">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/gordongwb.github.io"
    },
    "articleSection" : "posts",
    "name" : "LSTM Basic_Introduction",
    "headline" : "LSTM Basic_Introduction",
    "description" : "LSTM 由于梯度消失\/梯度爆炸的问题传统RNN在实际中很难处理长期依赖，而LSTM（Long Short Term Memory）则绕开了这些问题依然可以从语料中学习到长期依赖关系。\n传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作,如图：\nLSTM每个循环的模块内又有4层结构:3个sigmoid层，1个tanh层，如图：\nLSTM内部结构详解 LSTM的关键是细胞状态C，一条水平线贯穿于图形的上方，这条线上只有些少量的线性操作，信息在上面流传很容易保持。\n第一层是个忘记层，决定细胞状态中丢弃什么信息。把ht−1和xt拼接起来，传给一个sigmoid函数，该函数输出0到1之间的值，这个值乘到细胞状态Ct−1上去。sigmoid函数的输出值直接决定了状态信息保留多少。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。\n上一步的细胞状态Ct−1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项C~t，tanh的输出在[-1,1]上，说明细胞状态在某些维度上需要加强，在某些维度上需要减弱；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。\n现在可以让旧的细胞状态Ct−1与ft（f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分it∗C~t（i是input输入门的意思），这就生成了新的细胞状态CtCt。\n最后该决定输出什么了。输出值跟细胞状态有关，把Ct输给一个tanh函数得到输出值的候选项。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。\nLSTM 详解 单层ＬＳＴＭ结构如下\ncell = tf.contrib.rnn.LSTMBlockCell(hidden_size, forget_bias=0.0)\toutputs = [] state = self._initial_state # state with tf.variable_scope(\x26#34;RNN\x26#34;): for time_step in range(num_steps): if time_step \x26gt; 0: tf.get_variable_scope().reuse_variables() # cell_output: [batch_size,hidden_size] (cell_output, state) = cell(inputs[:,time_step,:], state) # outputs: a list: num_steps elements of shape [batch_size,hidden_size] outputs.append(cell_output) # output: first to shape:[batch_size,num_steps*hidden_size] and the first row is the data of the first sentense # and then reshpae to shape: [batch_size*num_steps,hidden_size], first num_steps rows is a sentense output = tf.",
    "inLanguage" : "en-us",
    "author" : "Gordon.Kwok",
    "creator" : "Gordon.Kwok",
    "publisher": "Gordon.Kwok",
    "accountablePerson" : "Gordon.Kwok",
    "copyrightHolder" : "Gordon.Kwok",
    "copyrightYear" : "2019",
    "datePublished": "2019-12-25 21:28:47 \x2b0800 CST",
    "dateModified" : "2019-12-25 21:28:47 \x2b0800 CST",
    "url" : "https:\/\/gordongwb.github.io\/2019\/lstm-basic-introduction\/",
    "wordCount" : "141",
    "keywords" : [ "ML","LSTM","RNN", "Gordon\x27s Blog"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://gordongwb.github.io">Gordon&#39;s Blog</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="About My Blog">About My Blog</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://gordongwb.github.io">Gordon&#39;s Blog</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">Blog</a>
                
                <a class="menu-item" href="/categories/" title="">Categories</a>
                
                <a class="menu-item" href="/tags/" title="">Tags</a>
                
                <a class="menu-item" href="/about/" title="About My Blog">About My Blog</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">LSTM Basic_Introduction</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://gordongwb.github.io" rel="author">Gordon.Kwok</a> with ♥ 
                <span class="post-time">
                on <time datetime=2019-12-25 itemprop="datePublished">December 25, 2019</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://gordongwb.github.io/categories/machine-learning/"> “Machine Learning” </a>
                        
                </span>
        </div>
    </header>
    <div class="post-content">
        

        
            
        

        
        
     
          
          
          

          
          
          

          <h1 id="lstm">LSTM</h1>
<p>由于梯度消失/梯度爆炸的问题传统RNN在实际中很难处理长期依赖，而LSTM（Long Short Term Memory）则绕开了这些问题依然可以从语料中学习到长期依赖关系。</p>
<p>传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作,如图：</p>
<p><img src="https://pic4.zhimg.com/80/v2-99a70ce82af523ed29a9a6f2122b59b7_hd.jpg" alt=""></p>
<p>LSTM每个循环的模块内又有4层结构:3个sigmoid层，1个tanh层，如图：</p>
<p><img src="https://pic1.zhimg.com/80/v2-29954c2e7292846ff142bd4be607fcb0_hd.jpg" alt=""></p>
<h2 id="lstm内部结构详解">LSTM内部结构详解</h2>
<p>LSTM的关键是细胞状态C，一条水平线贯穿于图形的上方，这条线上只有些少量的线性操作，信息在上面流传很容易保持。</p>
<p><img src="https://pic4.zhimg.com/80/v2-2b06468e314ff1cd64815fef103ca343_hd.jpg" alt=""></p>
<p>第一层是个忘记层，决定细胞状态中丢弃什么信息。把ht−1和xt拼接起来，传给一个sigmoid函数，该函数输出0到1之间的值，这个值乘到细胞状态Ct−1上去。sigmoid函数的输出值直接决定了状态信息保留多少。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。</p>
<p><img src="https://pic2.zhimg.com/80/v2-84c1933458ea8e746afe4b6c9487f701_hd.jpg" alt=""></p>
<p>上一步的细胞状态Ct−1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项C~t，tanh的输出在[-1,1]上，说明细胞状态在某些维度上需要加强，在某些维度上需要减弱；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个缩放的作用，极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p>
<p><img src="https://pic3.zhimg.com/80/v2-fc16c93b5ad7631a32694e5221514e5e_hd.jpg" alt=""></p>
<p>现在可以让旧的细胞状态Ct−1与ft（f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分it∗C~t（i是input输入门的意思），这就生成了新的细胞状态CtCt。</p>
<p><img src="https://pic3.zhimg.com/80/v2-6f41fdfa1f3b3dc4e59190c2d356d01a_hd.jpg" alt=""></p>
<p>最后该决定输出什么了。输出值跟细胞状态有关，把Ct输给一个tanh函数得到输出值的候选项。候选项中的哪些部分最终会被输出由一个sigmoid层来决定。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。</p>
<p><img src="https://pic4.zhimg.com/80/v2-235e1ceaf244329cbfc45878acec9d6b_hd.jpg" alt=""></p>
<h2 id="lstm-详解">LSTM 详解</h2>
<p>单层ＬＳＴＭ结构如下</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">cell <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>contrib<span style="color:#f92672">.</span>rnn<span style="color:#f92672">.</span>LSTMBlockCell(hidden_size, forget_bias<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>)	
outputs <span style="color:#f92672">=</span> []
state <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_initial_state <span style="color:#75715e"># state</span>
<span style="color:#66d9ef">with</span> tf<span style="color:#f92672">.</span>variable_scope(<span style="color:#e6db74"></span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">RNN</span><span style="color:#e6db74">&#34;</span>):
    <span style="color:#66d9ef">for</span> time_step <span style="color:#f92672">in</span> range(num_steps):
        <span style="color:#66d9ef">if</span> time_step <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>: tf<span style="color:#f92672">.</span>get_variable_scope()<span style="color:#f92672">.</span>reuse_variables()
            <span style="color:#75715e"># cell_output: [batch_size,hidden_size]</span>
            (cell_output, state) <span style="color:#f92672">=</span> cell(inputs[:,time_step,:], state) 
            <span style="color:#75715e"># outputs: a list: num_steps elements of shape [batch_size,hidden_size]</span>
            outputs<span style="color:#f92672">.</span>append(cell_output)  
            
<span style="color:#75715e"># output: first to shape:[batch_size,num_steps*hidden_size] and the first row is the data of the first sentense</span>
<span style="color:#75715e"># and then reshpae to shape: [batch_size*num_steps,hidden_size], first num_steps rows is a sentense</span>
output <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>reshape(tf<span style="color:#f92672">.</span>concat(outputs,<span style="color:#ae81ff">1</span>), [<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, hidden_size])

<span style="color:#75715e"># 7.Softmax: convert wordvec to probability for each word in vocab and calculate cross_entropy loss</span>
<span style="color:#75715e"># used to find which word in vocab the wordvec is like</span>
softmax_w <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_variable(<span style="color:#e6db74"></span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">softmax_w</span><span style="color:#e6db74">&#34;</span>, [hidden_size, vocab_size], dtype<span style="color:#f92672">=</span>data_type())
softmax_b <span style="color:#f92672">=</span> tf<span style="color:#f92672">.</span>get_variable(<span style="color:#e6db74"></span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">softmax_b</span><span style="color:#e6db74">&#34;</span>, [vocab_size], dtype<span style="color:#f92672">=</span>data_type())
</code></pre></div><p>用LSTMBlockCell构造了一个LSTM单元，单元里的隐藏单元个数是hidden_size，有四个神经网络，每个神经网络的输入是$h_t−1$和$x_t$，将它们concat到一起，维度为(hidden_size+wordvec_size)，所以LSTM里的每个黄框的参数矩阵的维度为
[hidden_size+wordvec_size,hidden_size]</p>
<p>需要注意的是，num_steps个时刻的LSTM都是共享一套参数的，说是有num_steps个LSTM单元，其实只有一个，只不过是对这个单元执行num_steps次。</p>
<p>上面的代码中有个for循环，是以时间进行展开，在循环里执行当前时刻下的单词。</p>
<p>举个例子，比如一批训练64句话，每句话20个单词，每个词向量长度为200，隐藏层单元个数为128</p>
<p>那么训练一批句子，输入的张量维度是[64,20,200]，$h_t$,$c_t$ 的维度是[128]，那么LSTM单元参数矩阵的维度是[128+200,4*128]，</p>
<p>在时刻1，把64句话的第一个单词作为输入，即输入一个[64,200]的矩阵，由于会和$h_t$进行concat，输入矩阵变成了[64,200+128]，输入矩阵会和参数矩阵[200+128,4<em>128]相乘，输出为[64,4</em>128]，也就是每个黄框的输出为[64,128]，黄框之间会进行一些操作，但不改变维度，输出依旧是[64,128]，即每个句子经过LSTM单元后，输出的维度是128，所以上一章节的每个LSTM输出的都是向量，包括$C_t$,$h_t$，它们的长度都是当前LSTM单元的hidden_size 得到了解释。那么我们就知道cell_output的维度为[64,128]</p>
<p>之后的时刻重复刚才同样的操作，那么outputs的维度是[20,64,128].</p>
<p>softmax相当于全连接层，将outputs映射到vocab_size个单词上，进行交叉熵误差计算。</p>
<p>然后根据误差更新LSTM参数矩阵和全连接层的参数。</p>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>Gordon.Kwok </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://gordongwb.github.io/2019/lstm-basic-introduction/>https://gordongwb.github.io/2019/lstm-basic-introduction/</span>
            </p>
            
             
            <p class="copyright-item lincese">
                本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
            </p>
            
    </div>

  
    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s): 
            
            <span class="tag"><a href="https://gordongwb.github.io/tags/ml/">
                    #ML</a></span>
            
            <span class="tag"><a href="https://gordongwb.github.io/tags/lstm/">
                    #LSTM</a></span>
            
            <span class="tag"><a href="https://gordongwb.github.io/tags/rnn/">
                    #RNN</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://gordongwb.github.io">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://gordongwb.github.io/2019/cnn-basic-introduction/" class="prev" rel="prev" title="CNN Basic Introduction"><i class="iconfont icon-left"></i>&nbsp;CNN Basic Introduction</a>
         
        
        <a href="https://gordongwb.github.io/2019/ctc-loss-basic-introduction/" class="next" rel="next" title="CTC LOSS Basic Introduction">CTC LOSS Basic Introduction&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2016 - 2020</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://gordongwb.github.io">Gordon.Kwok</a> | </span> 
         

         
		  <span>Powered by <a href="https://gohugo.io/" target="_blank" rel="external nofollow">Hugo</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span> 
    </div>
</footer>












    
     <link href="//lib.baomitu.com/lightgallery/1.6.11/css/lightgallery.min.css" rel="stylesheet">  
      
     <script src="/js/vendor_gallery.min.js" async="" ></script>
    
  



     </div>
  </body>
</html>
